# -*- coding: utf-8 -*-
"""CX_Support_RAG_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JZ8kOcdZ2XkNQrlta1nMXdnSsBSPbwA7

Goal:
Build a Q&A chatbot using:

Your support tickets dataset

Google Gemini for answering queries

FAISS for fast similarity search

LangChain to connect everything together
"""

#Install Req Libraries
!pip install -q langchain langchain-community langchain-core faiss-cpu tiktoken
!pip install google-ai-generativelanguage==0.6.15
!pip install langchain-google-genai

#Other Req Lib to Install
import os
import pandas as pd
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

#Set Up Google Gemini API Key
os.environ["GOOGLE_API_KEY"] = "AIzaSyC_4gVKqEBPcCKc6W5lAFlI9Z1HjrlA2PY"

GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"] #For Further Usage

#Initialize Embedding Model and LLM (Gemini)
embedding = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=GOOGLE_API_KEY
)

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.3,
    google_api_key=GOOGLE_API_KEY
)

#Load the Dataset
from google.colab import files
uploaded = files.upload()

#Read the CSV
import pandas as pd

df = pd.read_csv("customer_support_tickets.csv")
df[['Ticket ID', 'Ticket Subject', 'Ticket Description']].head()

#Convert Tickets into LangChain Documents
real_documents = []
for _, row in df.iterrows():
    text = f"Ticket Subject: {row['Ticket Subject']}\nTicket Description: {row['Ticket Description']}"
    doc = Document(page_content=text, metadata={"ticket_id": row["Ticket ID"]})
    real_documents.append(doc)

#Add Sample FAQs (Optional but Important)
sample_faqs = [
    {
        "Ticket ID": "FAQ001",
        "Ticket Subject": "How to update shipping address?",
        "Ticket Description": "To update your shipping address, log in to your account, go to Orders > Manage Orders > Edit Shipping Info."
    },
    {
        "Ticket ID": "FAQ002",
        "Ticket Subject": "How to cancel my order?",
        "Ticket Description": "To cancel your order, go to My Orders and click 'Cancel'. Cancellation is only allowed before the item is shipped."
    },
    {
        "Ticket ID": "FAQ003",
        "Ticket Subject": "What is the refund policy?",
        "Ticket Description": "We offer a full refund within 14 days of delivery if the product is returned in original condition."
    }
]

faq_documents = []
for faq in sample_faqs:
    text = f"Ticket Subject: {faq['Ticket Subject']}\nTicket Description: {faq['Ticket Description']}"
    doc = Document(page_content=text, metadata={"ticket_id": faq["Ticket ID"]})
    faq_documents.append(doc)

#Combine Real + FAQ Documents
all_documents = real_documents + faq_documents

#Chunk the Text for Better Retrieval
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
all_chunks = splitter.split_documents(all_documents)

#Build the FAISS Vector Index
vectorstore = FAISS.from_documents(all_chunks, embedding)

#Build the RetrievalQA Chain
qa_chain = RetrievalQA.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_type="similarity", k=3),
    return_source_documents=True
)

#Ask a Question:
query = "How do I update my shipping address for a recent order?"
result = qa_chain({"query": query})

#Display the Answer & Sources
print("üí¨ Answer:\n", result["result"])

print("\nüìö Sources:")
for doc in result["source_documents"]:
    print("-", doc.metadata)

"""**Customer Support RAG Bot with a Gradio Chatbot UI**"""

#Complete Backend Setup (FAISS + Gemini + RAG)
#import os
#import pandas as pd
#from langchain.schema import Document
#from langchain.text_splitter import RecursiveCharacterTextSplitter
#from langchain.vectorstores import FAISS
#from langchain.chains import RetrievalQA
#from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

# ‚úÖ Step 1: Set API key
#os.environ["GOOGLE_API_KEY"] = "your_gemini_api_key"
#GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"]

# ‚úÖ Step 2: Init models
#embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GOOGLE_API_KEY)
#llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.3, google_api_key=GOOGLE_API_KEY)

# ‚úÖ Step 3: Load dataset
#df = pd.read_csv("customer_support_tickets.csv")

# ‚úÖ Step 4: Real documents
#real_documents = []
#for _, row in df.iterrows():
#    text = f"Ticket Subject: {row['Ticket Subject']}\nTicket Description: {row['Ticket Description']}"
#    real_documents.append(Document(page_content=text, metadata={"ticket_id": row["Ticket ID"]}))

# ‚úÖ Step 5: Optional FAQs
#sample_faqs = [
#    {
#        "Ticket ID": "FAQ001",
#        "Ticket Subject": "How to update shipping address?",
#        "Ticket Description": "Log in to your account ‚Üí Orders ‚Üí Edit Shipping Info. If already shipped, contact support."
#    },
#]
#faq_documents = [Document(page_content=f"Ticket Subject: {faq['Ticket Subject']}\nTicket Description: {faq['Ticket Description']}", metadata={"ticket_id": faq["Ticket ID"]}) for faq in sample_faqs]

# ‚úÖ Step 6: Combine + Chunk
#all_documents = real_documents + faq_documents
#splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
#all_chunks = splitter.split_documents(all_documents)

# ‚úÖ Step 7: Create vector index
#vectorstore = FAISS.from_documents(all_chunks, embedding)

# ‚úÖ Step 8: Create RetrievalQA Chain
#qa_chain = RetrievalQA.from_llm(
#    llm=llm,
#    retriever=vectorstore.as_retriever(search_type="similarity", k=3),
#    return_source_documents=True
#)

#Add Gradio Chatbot UI
import gradio as gr

def chatbot_response(user_input):
    try:
        result = qa_chain({"query": user_input})
        response = result["result"]

        sources = "\n\nüìö Sources:\n" + "\n".join(
            [f"- {doc.metadata['ticket_id']}" for doc in result["source_documents"]]
        )
        return response + sources
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

with gr.Blocks() as demo:
    gr.Markdown("## ü§ñ Customer Support RAG Chatbot (Gemini + FAISS)")
    chatbot = gr.Chatbot(type="messages")  # ‚úÖ fixed deprecated tuple warning
    txt = gr.Textbox(placeholder="Ask a support question and press enter...")

    def respond(user_message, history):
        answer = chatbot_response(user_message)
        history = history + [{"role": "user", "content": user_message}, {"role": "assistant", "content": answer}]
        return history, ""

    txt.submit(respond, [txt, chatbot], [chatbot, txt])

demo.launch()